---
title: Docking Benchmark Group
subtitle:
layout: page
show_sidebar: false
menubar: lb_docking_group
hide_hero: true
---

## Docking Molecule Generation Benchmark Group

### Introduction

<p class="is-size-5"> AI-assisted molecule generation aims to generate novel molecular structures with desired properties. Current techniques for evaluating the quality of generated molecules focus on heuristic oracles, such as QED, LogP, and DRD2, and do not reflect the complexity of the real-world environment. This creates several key challenges.  Many properties, such as binding propensity towards a target protein, are incredibly resource-intensive to investigate through experiments or computational simulations. For this reason, techniques that require a large number of calls to oracles do not constitute a practical strategy to evaluate the generated molecules. Even in settings where generated molecules score highly by some oracles, the generated molecules may not have other necessary properties (e.g., low synthesizability) to constitute promising therapeutic candidates.
</p>

<p class="is-size-5"> To address these challenges, we designed a docking benchmark group [<a href="https://arxiv.org/abs/2006.16955">Cieplinski et al. 2020</a>, <a href="https://chemrxiv.org/articles/preprint/Using_a_Genetic_Algorithm_to_Find_Molecules_with_Good_Docking_Scores/13525589/2#:~:text=A%20graph%2Dbased%20genetic%20algorithm,2%2Dadrenergic%20G%20protein%2Dcoupled">Steinmann and Jensen, 2021</a>]. Docking is a theoretical evaluation of affinity between a ligand (a small molecular drug) and a target (a protein involved in the disease). As a molecule with higher affinity is more likely to have higher bioactivity, docking is widely used for virtual screening of compounds [<a href="https://www.nature.com/articles/s41586-019-0917-9?fbclid=IwAR1HDXx0kEsNlRQZXVtPkmX7hU_gDoT2aqVEiBZj04qhz_6x1WCbNkj75IE">Lyu et al. 2020</a>]. 
</p>

<p class="is-size-5">
This benchmark evaluates generated molecules against their affinity to the target protein (i.e., quantified through docking scores). To this end, the benchmark is structured as follows:
<ul class="is-size-5">
<li> As docking scores are relatively costly to calculate, we <i>restrict the number of oracle calls</i> in this benchmark requiring the models to adapt quickly. This setup simulates a real-world environment with a limited number of wet-lab experiments that can be carried out.</li>
<li>In addition to typical oracle scores, we provide <i>additional tests</i> to evaluate how realistic the generated molecules are.</li>
</ul>
</p> 

<p class="is-size-5">
The training dataset originates from <a href="/generation_tasks/molgen/#zinc">ZINC 250K</a>.
</p>

### Accessing the Dataset

<p class="is-size-5"> To retrieve the names of benchmarks constitute this benchmark group, type the following: </p>

```python
from tdc import utils
names = utils.retrieve_benchmark_names('Docking_Group')
# ['DRD3', 'XXX', ...]
```

<p class="is-size-5">To access a benchmark, use the following code:</p>

```python
from tdc.benchmark_group import docking_group
group = docking_group(path = 'data/', 
                file_format='oracle', 
                pyscreener_path = 'PATH_TO_PyScreener')

benchmark = group.get('DRD3', num_max_call = 5000) 
# specify the number of maximum calls your model plans to use 

predictions = {}
oracle_fct, data, name = benchmark['oracle'], benchmark['data'], benchmark['name'] 

# --------------------------------------------- # 
#  Train your model using oracle_fct and data   #
#    Save SMILES generation in pred_smiles      #
# --------------------------------------------- #

predictions[name] = pred_smiles 
'''
pred_smiles format is a dictionary of the top 100 generated SMILES: 
{5000: {
  'C=C=C=C(C#CON(N=O)C(=NO)C(O)=NC#CC)C(N=CC)(ON=NO)C(=CNN=NNCC)OO': -6.0,
  'CNN=CC(CC(O)(CCN(O)O)NC(=N)C(C)=COO)OC(=O)N=NON=C(O)CC=CNO': -7.2,
  ...
  '(O)OC#CNOC(N)C(=O)NOC(CN)C1=C=C=C=C1': -9.8
 },
 ### if you also evaluate on 1000/500/100 maximum calls
 1000: {
  .....
 }
}

Note: if no docking score is generated by the model for the final 
100 SMILES, you can also input a list of 100 generated SMILES and 
TDC will call the docking scores in the evaluate function.
'''

out = group.evaluate(predictions, m1_api = 'XXXXX')
'''
{'DRD3': {5000: {'top100': -10.2,
          'top10': -11.3,
          'top1': -12.3,
          'diversity': 0.6,
          'novelty': 0.7,
          '%pass': 0.7,
          'top1_%pass': -11.2,
          'm1': 2.5,
          'top smiles': ['XXX', 'XXX', ...]
          },
        1000: {....
          }
        }          
}

Note that if you put save_dict = True in evaluate function, it 
would return more detailed evaluation outcomes, namely the list of 
smiles pass the filter, a dictionary of smiles with m1 scores and docking scores.

We ask users to submit at least three random runs of models for robustness. 
You can use following functions to obtain submission ready format:
'''
predictions_runs = [pred_run1, pred_run2, pred_run3]
out = group.evaluate_many(predictions_runs, save_file_name = 'result', m1_api = 'XXXXX')
'''
{'DRD3': {5000: {'top100': [-10.2, 0.12],
          'top10': [-11.3, 0.01],
          'top1': [-12.3, 0.02],
          'diversity': [0.6, 0.001],
          'novelty': [0.7, 0.01],
          '%pass': [0.7, 0.02],
          'top1_%pass': [-11.2, 0.03],
          'm1': [5.5, 0.04],
          'top smiles': ['XXX', 'XXX', ...] # superset of runs
          },
        1000: {....
          }
        }          
}

In default, this evaluate_many function will call group.evaluate for each run. If you have the evaluate result for each fold, simply specify it with 'results_individual = XX' to skip the evaluation calls.
'''
```

### Performance Evaluation

<p class="is-size-5"> To evaluate the quality of generated molecules, we report the following metrics (average and standard deviation across 3 or more independent runs): </p>

<ul class="is-large is-size-5">
  <li><code>top100</code>: Average docking score of top-100 generated molecules for a given target.</li>
  <li><code>top10</code>: Average docking score of top-10 generated molecules for a given target.</li>
  <li><code>top1</code>: The lowest docking score of generated molecules.</li>
  <li><code>diversity</code>: Average pairwise Tanimoto distance of Morgan fingerprints for top-100 generated molecules.</li>
  <li><code>novelty</code>: Fraction of generated molecules that are not present in the training set.</li>
  <li><code>m1</code>: Synthesizability score of molecules obtained via molecule.one retrosynthesis model. </li>
  <li><code>%pass</code>: Fraction of generated molecules that successfully pass through a-priori defined filters. These filters are rules compiled by medicinal chemists and test whether compounds are promising candidates for downstream analyses.</li>
  <li><code>top1_%pass (top1-p)</code>: The lowest docking score for molecules molecules that are not filtered out.</li>
  <li><code>molecules</code>: Visualizations of molecular structure of the superset of top-100 molecules across independent runs of the model.</li> 
</ul>

<p class="is-size-5">Note that all evaluations are automatically computed by <code>group.evaluate</code> function with the exception of <code>m1</code> evaluation metric.</p> 

<p class="is-size-5">To include <code>m1</code> in the evaluation, specify the <code>m1_api</code> token in the evaluation function. Note that this is a non-commercial service kindly provided to TDC by our partner organization <a href="http://molecule.one/">Molecule.one</a>. Please follow Molecule.one's terms of usage if you plan to calculate <code>m1</code> scores. You can opt-out of <code>m1</code> and submit your results without it, by not specifying the <code>m1_api</code> token. Check out this <a href="/functions/oracles/#moleculeone"> page </a> to get the API token and learn about terms of usage. </p>

### Maximum Number of Calls to Oracles

<p class="is-size-5">
To simulate the resource-intensiveness in real-world molecule generation, we restrict the number of maximum calls to oracles in O(10<sup>3</sup>). To evaluate models in increasingly harder learning regimes, we provide four leaderboards, each allowing only a certain number of oracle calls. The smaller the number of allowed oracle calls, the harder is the learning task. You can specify the maximum number of oracle calls as follows: <code>group.get('DRD3', num_max_call = 5000)</code>. We currently support leaderboards with a maximum of 100 (toughest learning regime), 500, 1000, or 5000 (the least tough learning regime) calls to the oracles.
</p>

### Instructions

<p class="is-size-5">This benchmark requires the use of TDC oracle class. For docking score, it requires PyScreener. You can find the detailed installation steps <a href="/functions/oracles/#docking-scores">here</a>. </p>

<p class="is-size-5"> To submit your result, please fill out <b><a href="https://forms.gle/HYupGaV7WDuutbr9A">THIS FORM</a></b>. The evaluation result file <code>result.pkl</code> will be automatically generated after calling <code>group.evaluate_many(predictions_runs, save_file_name = 'result', m1_api = 'XXXXX')</code>. </p>


<div class="column is-12">
    <hr />
</div>

### Leaderboard Data Summary

<hr />

<table class="table is-striped is-hoverable">
  <thead>
  <tr>
    <th>Dataset</th>
    <th>Diseases</th>
    <th>Link to DRD3 target protein</th>
  </tr>
  </thead>
  <tr>
    <td><code>TDC.DRD3</code></td>
    <td>Tremor, Schizophrenia</td>
    <td><a href="https://www.uniprot.org/uniprot/P35462">Uniprot Page</a></td>
  </tr>
</table>
